{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from rl_model.data_reader import Data_Reader\n",
    "import rl_model.data_parser\n",
    "import config\n",
    "\n",
    "from rl_model.model import Seq2Seq_chatbot\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Global Parameters ###\n",
    "checkpoint = config.CHECKPOINT\n",
    "model_path = config.train_model_path\n",
    "model_name = config.train_model_name\n",
    "start_epoch = config.start_epoch\n",
    "\n",
    "word_count_threshold = config.WC_threshold\n",
    "\n",
    "### Train Parameters ###\n",
    "dim_wordvec = 300\n",
    "dim_hidden = 1000\n",
    "\n",
    "n_encode_lstm_step = 22 + 22\n",
    "n_decode_lstm_step = 22\n",
    "\n",
    "epochs = 500\n",
    "batch_size = 100\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training functions \n",
    "def pad_sequences(sequences, maxlen=None, dtype='int32', padding='pre', truncating='pre', value=0.):\n",
    "    if not hasattr(sequences, '__len__'):\n",
    "        raise ValueError('`sequences` must be iterable.')\n",
    "    lengths = []\n",
    "    for x in sequences:\n",
    "        if not hasattr(x, '__len__'):\n",
    "            raise ValueError('`sequences` must be a list of iterables. '\n",
    "                             'Found non-iterable: ' + str(x))\n",
    "        lengths.append(len(x))\n",
    "\n",
    "    num_samples = len(sequences)\n",
    "    if maxlen is None:\n",
    "        maxlen = np.max(lengths)\n",
    "\n",
    "    # take the sample shape from the first non empty sequence\n",
    "    # checking for consistency in the main loop below.\n",
    "    sample_shape = tuple()\n",
    "    for s in sequences:\n",
    "        if len(s) > 0:\n",
    "            sample_shape = np.asarray(s).shape[1:]\n",
    "            break\n",
    "\n",
    "    x = (np.ones((num_samples, maxlen) + sample_shape) * value).astype(dtype)\n",
    "    for idx, s in enumerate(sequences):\n",
    "        if not len(s):\n",
    "            continue  # empty list/array was found\n",
    "        if truncating == 'pre':\n",
    "            trunc = s[-maxlen:]\n",
    "        elif truncating == 'post':\n",
    "            trunc = s[:maxlen]\n",
    "        else:\n",
    "            raise ValueError('Truncating type \"%s\" not understood' % truncating)\n",
    "\n",
    "        # check `trunc` has expected shape\n",
    "        trunc = np.asarray(trunc, dtype=dtype)\n",
    "        if trunc.shape[1:] != sample_shape:\n",
    "            raise ValueError('Shape of sample %s of sequence at position %s is different from expected shape %s' %\n",
    "                             (trunc.shape[1:], idx, sample_shape))\n",
    "\n",
    "        if padding == 'post':\n",
    "            x[idx, :len(trunc)] = trunc\n",
    "        elif padding == 'pre':\n",
    "            x[idx, -len(trunc):] = trunc\n",
    "        else:\n",
    "            raise ValueError('Padding type \"%s\" not understood' % padding)\n",
    "    return x\n",
    "\n",
    "def train():\n",
    "    wordtoix, ixtoword, bias_init_vector = data_parser.preProBuildWordVocab(word_count_threshold=word_count_threshold)\n",
    "    word_vector = KeyedVectors.load_word2vec_format('model/word_vector.bin', binary=True)\n",
    "\n",
    "    model = Seq2Seq_chatbot(\n",
    "            dim_wordvec=dim_wordvec,\n",
    "            n_words=len(wordtoix),\n",
    "            dim_hidden=dim_hidden,\n",
    "            batch_size=batch_size,\n",
    "            n_encode_lstm_step=n_encode_lstm_step,\n",
    "            n_decode_lstm_step=n_decode_lstm_step,\n",
    "            bias_init_vector=bias_init_vector,\n",
    "            lr=learning_rate)\n",
    "\n",
    "    train_op, tf_loss, word_vectors, tf_caption, tf_caption_mask, inter_value = model.build_model()\n",
    "\n",
    "    saver = tf.train.Saver(max_to_keep=100)\n",
    "\n",
    "    sess = tf.InteractiveSession()\n",
    "    \n",
    "    if checkpoint:\n",
    "        print(\"Use Model {}.\".format(model_name))\n",
    "        saver.restore(sess, os.path.join(model_path, model_name))\n",
    "        print(\"Model {} restored.\".format(model_name))\n",
    "    else:\n",
    "        print(\"Restart training...\")\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "    dr = Data_Reader()\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        n_batch = dr.get_batch_num(batch_size)\n",
    "        for batch in range(n_batch):\n",
    "            start_time = time.time()\n",
    "\n",
    "            batch_X, batch_Y = dr.generate_training_batch(batch_size)\n",
    "\n",
    "            for i in range(len(batch_X)):\n",
    "                batch_X[i] = [word_vector[w] if w in word_vector else np.zeros(dim_wordvec) for w in batch_X[i]]\n",
    "                # batch_X[i].insert(0, np.random.normal(size=(dim_wordvec,))) # insert random normal at the first step\n",
    "                if len(batch_X[i]) > n_encode_lstm_step:\n",
    "                    batch_X[i] = batch_X[i][:n_encode_lstm_step]\n",
    "                else:\n",
    "                    for _ in range(len(batch_X[i]), n_encode_lstm_step):\n",
    "                        batch_X[i].append(np.zeros(dim_wordvec))\n",
    "\n",
    "            current_feats = np.array(batch_X)\n",
    "\n",
    "            current_captions = batch_Y\n",
    "            current_captions = map(lambda x: '<bos> ' + x, current_captions)\n",
    "            current_captions = map(lambda x: x.replace('.', ''), current_captions)\n",
    "            current_captions = map(lambda x: x.replace(',', ''), current_captions)\n",
    "            current_captions = map(lambda x: x.replace('\"', ''), current_captions)\n",
    "            current_captions = map(lambda x: x.replace('\\n', ''), current_captions)\n",
    "            current_captions = map(lambda x: x.replace('?', ''), current_captions)\n",
    "            current_captions = map(lambda x: x.replace('!', ''), current_captions)\n",
    "            current_captions = map(lambda x: x.replace('\\\\', ''), current_captions)\n",
    "            current_captions = map(lambda x: x.replace('/', ''), current_captions)\n",
    "\n",
    "            for idx, each_cap in enumerate(current_captions):\n",
    "                word = each_cap.lower().split(' ')\n",
    "                if len(word) < n_decode_lstm_step:\n",
    "                    current_captions[idx] = current_captions[idx] + ' <eos>'\n",
    "                else:\n",
    "                    new_word = ''\n",
    "                    for i in range(n_decode_lstm_step-1):\n",
    "                        new_word = new_word + word[i] + ' '\n",
    "                    current_captions[idx] = new_word + '<eos>'\n",
    "\n",
    "            current_caption_ind = []\n",
    "            for cap in current_captions:\n",
    "                current_word_ind = []\n",
    "                for word in cap.lower().split(' '):\n",
    "                    if word in wordtoix:\n",
    "                        current_word_ind.append(wordtoix[word])\n",
    "                    else:\n",
    "                        current_word_ind.append(wordtoix['<unk>'])\n",
    "                current_caption_ind.append(current_word_ind)\n",
    "\n",
    "            current_caption_matrix = pad_sequences(current_caption_ind, padding='post', maxlen=n_decode_lstm_step)\n",
    "            current_caption_matrix = np.hstack([current_caption_matrix, np.zeros([len(current_caption_matrix), 1])]).astype(int)\n",
    "            current_caption_masks = np.zeros((current_caption_matrix.shape[0], current_caption_matrix.shape[1]))\n",
    "            nonzeros = np.array(map(lambda x: (x != 0).sum() + 1, current_caption_matrix))\n",
    "\n",
    "            for ind, row in enumerate(current_caption_masks):\n",
    "                row[:nonzeros[ind]] = 1\n",
    "\n",
    "            if batch % 100 == 0:\n",
    "                _, loss_val = sess.run(\n",
    "                        [train_op, tf_loss],\n",
    "                        feed_dict={\n",
    "                            word_vectors: current_feats,\n",
    "                            tf_caption: current_caption_matrix,\n",
    "                            tf_caption_mask: current_caption_masks\n",
    "                        })\n",
    "                print(\"Epoch: {}, batch: {}, loss: {}, Elapsed time: {}\".format(epoch, batch, loss_val, time.time() - start_time))\n",
    "            else:\n",
    "                _ = sess.run(train_op,\n",
    "                             feed_dict={\n",
    "                                word_vectors: current_feats,\n",
    "                                tf_caption: current_caption_matrix,\n",
    "                                tf_caption_mask: current_caption_masks\n",
    "                            })\n",
    "\n",
    "\n",
    "        print(\"Epoch \", epoch, \" is done. Saving the model ...\")\n",
    "        saver.save(sess, os.path.join(model_path, 'model'), global_step=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model - takes roughly 9 hours \n",
    "train() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#=====================================================\n",
    "# Global Parameters\n",
    "#=====================================================\n",
    "default_model_path = './model/Seq2Seq/model-77'\n",
    "testing_data_path = 'sample_input.txt' if len(sys.argv) <= 2 else sys.argv[2]\n",
    "output_path = 'sample_output_S2S.txt' if len(sys.argv) <= 3 else sys.argv[3]\n",
    "\n",
    "word_count_threshold = config.WC_threshold\n",
    "\n",
    "#=====================================================\n",
    "# Train Parameters\n",
    "#=====================================================\n",
    "dim_wordvec = 300\n",
    "dim_hidden = 1000\n",
    "\n",
    "n_encode_lstm_step = 22 + 1 # one random normal as the first timestep\n",
    "n_decode_lstm_step = 22\n",
    "\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Testing functions\n",
    "\"\"\" Extract only the vocabulary part of the data \"\"\"\n",
    "def refine(data):\n",
    "    words = re.findall(\"[a-zA-Z'-]+\", data)\n",
    "    words = [\"\".join(word.split(\"'\")) for word in words]\n",
    "    # words = [\"\".join(word.split(\"-\")) for word in words]\n",
    "    data = ' '.join(words)\n",
    "    return data\n",
    "\n",
    "def test(model_path=default_model_path):\n",
    "    testing_data = open(testing_data_path, 'r').read().split('\\n')\n",
    "\n",
    "    word_vector = KeyedVectors.load_word2vec_format('model/word_vector.bin', binary=True)\n",
    "\n",
    "    _, ixtoword, bias_init_vector = data_parser.preProBuildWordVocab(word_count_threshold=word_count_threshold)\n",
    "\n",
    "    model = Seq2Seq_chatbot(\n",
    "            dim_wordvec=dim_wordvec,\n",
    "            n_words=len(ixtoword),\n",
    "            dim_hidden=dim_hidden,\n",
    "            batch_size=batch_size,\n",
    "            n_encode_lstm_step=n_encode_lstm_step,\n",
    "            n_decode_lstm_step=n_decode_lstm_step,\n",
    "            bias_init_vector=bias_init_vector)\n",
    "\n",
    "    word_vectors, caption_tf, probs, _ = model.build_generator()\n",
    "\n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    try:\n",
    "        print('\\n=== Use model', model_path, '===\\n')\n",
    "        saver.restore(sess, model_path)\n",
    "    except:\n",
    "        print('\\nUse default model\\n')\n",
    "        saver.restore(sess, default_model_path)\n",
    "\n",
    "    with open(output_path, 'w') as out:\n",
    "        generated_sentences = []\n",
    "        bleu_score_avg = [0., 0.]\n",
    "        for idx, question in enumerate(testing_data):\n",
    "            print('question =>', question)\n",
    "\n",
    "            question = [refine(w) for w in question.lower().split()]\n",
    "            question = [word_vector[w] if w in word_vector else np.zeros(dim_wordvec) for w in question]\n",
    "            question.insert(0, np.random.normal(size=(dim_wordvec,))) # insert random normal at the first step\n",
    "\n",
    "            if len(question) > n_encode_lstm_step:\n",
    "                question = question[:n_encode_lstm_step]\n",
    "            else:\n",
    "                for _ in range(len(question), n_encode_lstm_step):\n",
    "                    question.append(np.zeros(dim_wordvec))\n",
    "\n",
    "            question = np.array([question]) # 1x22x300\n",
    "    \n",
    "            generated_word_index, prob_logit = sess.run([caption_tf, probs], feed_dict={word_vectors: question})\n",
    "            \n",
    "            # remove <unk> to second high prob. word\n",
    "            for i in range(len(generated_word_index)):\n",
    "                if generated_word_index[i] == 3:\n",
    "                    sort_prob_logit = sorted(prob_logit[i][0])\n",
    "                    maxindex = np.where(prob_logit[i][0] == sort_prob_logit[-1])[0][0]\n",
    "                    secmaxindex = np.where(prob_logit[i][0] == sort_prob_logit[-2])[0][0]\n",
    "                    generated_word_index[i] = secmaxindex\n",
    "\n",
    "            generated_words = []\n",
    "            for ind in generated_word_index:\n",
    "                generated_words.append(ixtoword[ind])\n",
    "\n",
    "            # generate sentence\n",
    "            punctuation = np.argmax(np.array(generated_words) == '<eos>') + 1\n",
    "            generated_words = generated_words[:punctuation]\n",
    "            generated_sentence = ' '.join(generated_words)\n",
    "\n",
    "            # modify the output sentence \n",
    "            generated_sentence = generated_sentence.replace('<bos> ', '')\n",
    "            generated_sentence = generated_sentence.replace(' <eos>', '')\n",
    "            generated_sentence = generated_sentence.replace('--', '')\n",
    "            generated_sentence = generated_sentence.split('  ')\n",
    "            for i in range(len(generated_sentence)):\n",
    "                generated_sentence[i] = generated_sentence[i].strip()\n",
    "                if len(generated_sentence[i]) > 1:\n",
    "                    generated_sentence[i] = generated_sentence[i][0].upper() + generated_sentence[i][1:] + '.'\n",
    "                else:\n",
    "                    generated_sentence[i] = generated_sentence[i].upper()\n",
    "            generated_sentence = ' '.join(generated_sentence)\n",
    "            generated_sentence = generated_sentence.replace(' i ', ' I ')\n",
    "            generated_sentence = generated_sentence.replace(\"i'm\", \"I'm\")\n",
    "            generated_sentence = generated_sentence.replace(\"i'd\", \"I'd\")\n",
    "            generated_sentence = generated_sentence.replace(\"i'll\", \"I'll\")\n",
    "            generated_sentence = generated_sentence.replace(\"i'v\", \"I'v\")\n",
    "            generated_sentence = generated_sentence.replace(\" - \", \"\")\n",
    "\n",
    "            print('generated_sentence =>', generated_sentence)\n",
    "            out.write(generated_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run testing functions\n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
